{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374db1cf-46f5-4cfe-a765-3ef755e976b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Model name (without extension):  test_full_sentense\n",
      "Camera source number (e.g., 0):  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-05-28 22:57:40.736 python[8170:247683] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748462263.332546  247683 gl_context.cc:369] GL version: 2.1 (2.1 ATI-7.0.3), renderer: AMD Radeon Pro 555X OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748462263.357220  248412 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748462263.382681  248412 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748462264.598079  248414 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "import tempfile\n",
    "import threading\n",
    "\n",
    "import requests\n",
    "import pygame\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# ElevenLabs API ---\n",
    "API_KEY = 'sk_e883cf78c9b9e06bb7236ca8ba711d0f5fe77281579f616b'\n",
    "VOICE_ID = 'wxweiHvoC2r2jFM7mS8b'\n",
    "\n",
    "def speak_elevenlabs(text):\n",
    "    def thread_speak():\n",
    "        try:\n",
    "            url = f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}\"\n",
    "            headers = {\n",
    "                \"Accept\": \"audio/mpeg\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"xi-api-key\": API_KEY\n",
    "            }\n",
    "            data = {\n",
    "                \"text\": text,\n",
    "                \"model_id\": \"eleven_multilingual_v2\",\n",
    "                \"voice_settings\": {\n",
    "                    \"stability\": 0.5,\n",
    "                    \"similarity_boost\": 0.8\n",
    "                }\n",
    "            }\n",
    "\n",
    "            response = requests.post(url, json=data, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as fp:\n",
    "                fp.write(response.content)\n",
    "                fp.flush()\n",
    "                pygame.mixer.init()\n",
    "                pygame.mixer.music.load(fp.name)\n",
    "                pygame.mixer.music.play()\n",
    "                while pygame.mixer.music.get_busy():\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"حدث خطأ أثناء استخدام ElevenLabs: {e}\")\n",
    "\n",
    "    threading.Thread(target=thread_speak).start()\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "class SignLanguageInterpreter:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = load_model(f'{model_path}.h5')\n",
    "        self.model.compile()\n",
    "\n",
    "        with open(f'{model_path}_actions.txt', encoding='utf-8') as f:\n",
    "            self.actions = np.array([line.strip() for line in f if line.strip()])\n",
    "\n",
    "        self.scaler_mean = np.load(f'{model_path}_scaler_mean.npy')\n",
    "        self.scaler_scale = np.load(f'{model_path}_scaler_scale.npy')\n",
    "\n",
    "        self.no_frames = 30\n",
    "        self.stability_window = 12\n",
    "        self.confidence_threshold = 0.85\n",
    "        self.sequence = deque(maxlen=self.no_frames)\n",
    "        self.prediction_buffer = deque(maxlen=self.stability_window)\n",
    "        self.sentence = []\n",
    "        self.last_prediction_time = 0\n",
    "        self.cooldown_period = 2.0\n",
    "        self.visual_flash = 0\n",
    "\n",
    "    def scale_features(self, features):\n",
    "        return (features - self.scaler_mean) / self.scaler_scale\n",
    "\n",
    "    def predict(self):\n",
    "        if len(self.sequence) == self.no_frames:\n",
    "            scaled_sequence = self.scale_features(np.array(self.sequence))\n",
    "            res = self.model.predict(np.expand_dims(scaled_sequence, axis=0), verbose=0)[0]\n",
    "            confidence = np.max(res)\n",
    "            predicted_class = np.argmax(res)\n",
    "            return predicted_class, confidence\n",
    "        return None, 0\n",
    "\n",
    "    def update_state(self, predicted_class, confidence):\n",
    "        current_time = time.time()\n",
    "        if confidence >= self.confidence_threshold:\n",
    "            self.prediction_buffer.append(predicted_class)\n",
    "            if len(self.prediction_buffer) == self.stability_window:\n",
    "                common = Counter(self.prediction_buffer).most_common(1)[0]\n",
    "                if common[1] >= int(0.8 * self.stability_window):\n",
    "                    final_pred = common[0]\n",
    "                    predicted_word = self.actions[final_pred]\n",
    "                    if (not self.sentence or \n",
    "                        predicted_word != self.sentence[-1] or \n",
    "                        current_time - self.last_prediction_time > self.cooldown_period):\n",
    "                        self.last_prediction_time = current_time\n",
    "                        self.sentence.append(predicted_word)\n",
    "                        self.prediction_buffer.clear()\n",
    "                        self.sequence.clear()  # <=== هذا السطر يمنع تكرار الإشارة السابقة\n",
    "                        self.visual_flash = 3  # set flash counter\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def reset_after_no_hand(self):\n",
    "        self.sequence.clear()\n",
    "        self.prediction_buffer.clear()\n",
    "\n",
    "    def get_current_sentence(self, max_length=5):\n",
    "        return '  '.join(self.sentence[-max_length:])\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2BGR), results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.zeros(21*3)\n",
    "    rh = np.zeros(21*3)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            handedness = results.multi_handedness[idx].classification[0].label\n",
    "            landmarks = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "            if handedness == 'Right':\n",
    "                rh = landmarks\n",
    "            else:\n",
    "                lh = landmarks\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def draw_confidence_bar(image, confidence):\n",
    "    bar_width = 200\n",
    "    bar_height = 20\n",
    "    fill_width = int(bar_width * confidence)\n",
    "    cv2.rectangle(image, (10, 60), (10 + bar_width, 60 + bar_height), (255, 255, 255), 1)\n",
    "    cv2.rectangle(image, (10, 60), (10 + fill_width, 60 + bar_height), (0, 255, 0), -1)\n",
    "    cv2.putText(image, f'Confidence: {confidence:.2f}', (10, 55),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "def draw_arabic_text(image, text, position, font_size=36):\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # تأكد أن الخط موجود في جهازك، وغير المسار حسب موقع الخط\n",
    "    font_path = \"/Users/mac/Desktop/LISAN AL ISHARA1/NotoNaskhArabic-Regular.ttf\"  \n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    img_pil = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.text(position, bidi_text, font=font, fill=(255, 255, 255))\n",
    "    return np.array(img_pil)\n",
    "\n",
    "start_clicked = False\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global start_clicked\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if 250 <= x <= 390 and 200 <= y <= 260:\n",
    "            start_clicked = True\n",
    "\n",
    "def draw_start_button(frame):\n",
    "    cv2.rectangle(frame, (250, 200), (390, 260), (0, 255, 0), -1)\n",
    "    cv2.putText(frame, 'START', (260, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "def wait_for_start():\n",
    "    global start_clicked\n",
    "    cv2.namedWindow(\"Sign Language Interpreter\")\n",
    "    cv2.setMouseCallback(\"Sign Language Interpreter\", mouse_callback)\n",
    "    dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    while not start_clicked:\n",
    "        frame = dummy_frame.copy()\n",
    "        draw_start_button(frame)\n",
    "        draw_arabic_text(frame, \"اضغط على زر Start لبدء الترجمة\", (100, 150), 28)\n",
    "        cv2.imshow(\"Sign Language Interpreter\", frame)\n",
    "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.setMouseCallback(\"Sign Language Interpreter\", lambda *args: None)\n",
    "\n",
    "def main():\n",
    "    model_path = input(\"Model name (without extension): \").strip()\n",
    "    no_cam = int(input(\"Camera source number (e.g., 0): \"))\n",
    "\n",
    "    interpreter = SignLanguageInterpreter(model_path)\n",
    "    wait_for_start()\n",
    "\n",
    "    cap = cv2.VideoCapture(no_cam)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    last_hand_time = time.time()\n",
    "\n",
    "    with mp_hands.Hands(\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.75,\n",
    "        min_tracking_confidence=0.7\n",
    "    ) as hands:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                continue\n",
    "\n",
    "            image, results = mediapipe_detection(frame, hands)\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            keypoints = extract_keypoints(results)\n",
    "            prediction_text = \"\"\n",
    "            confidence = 0\n",
    "\n",
    "            hand_detected = not np.all(keypoints == 0)\n",
    "\n",
    "            if hand_detected:\n",
    "                last_hand_time = time.time()\n",
    "                interpreter.sequence.append(keypoints)\n",
    "                predicted_class, confidence = interpreter.predict()\n",
    "                if predicted_class is not None:\n",
    "                    updated = interpreter.update_state(predicted_class, confidence)\n",
    "                    prediction_text = interpreter.actions[predicted_class]\n",
    "            else:\n",
    "                if time.time() - last_hand_time > 3.0:\n",
    "                    interpreter.reset_after_no_hand()\n",
    "                    if interpreter.sentence:\n",
    "                        full_sentence = ' '.join(interpreter.sentence)\n",
    "                        speak_elevenlabs(full_sentence)\n",
    "                        interpreter.sentence.clear()\n",
    "\n",
    "            overlay = image.copy()\n",
    "            cv2.rectangle(overlay, (0, 400), (640, 480), (0, 0, 0), -1)\n",
    "            image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)\n",
    "\n",
    "            sentence_display = interpreter.get_current_sentence().replace(' ', ' ')\n",
    "            image = draw_arabic_text(image, sentence_display, (10, 420), 36)\n",
    "\n",
    "            if confidence > 0:\n",
    "                draw_confidence_bar(image, confidence)\n",
    "\n",
    "            if interpreter.visual_flash > 0:\n",
    "                cv2.rectangle(image, (0, 0), (640, 480), (0, 255, 255), thickness=15)\n",
    "                interpreter.visual_flash -= 1\n",
    "\n",
    "            cv2.imshow(\"Sign Language Interpreter\", image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddafe9be-9022-42b1-b5ac-3380cb78d48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b9b03-c17d-4c69-996b-d886b7db3683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
